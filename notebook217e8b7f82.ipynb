{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U \\\n  nest_asyncio \\\n  fastapi \\\n  uvicorn \\\n  pyngrok \\\n  yt-dlp \\\n  moviepy \\\n  langdetect \\\n  requests \\\n  transformers \\\n  torch \\\n  torchvision \\\n  torchaudio \\\n  sentencepiece \\\n  git+https://github.com/openai/whisper.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch==2.0.1+cu117 torchvision==0.15.2+cu117 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu117","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade git+https://github.com/openai/whisper.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(\"CUDA available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"Device:\", torch.cuda.get_device_name(0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð½ÐµÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ñ‹Ðµ Ð²ÐµÑ€ÑÐ¸Ð¸\n!pip uninstall -y torch torchvision torchaudio\n\n# Ð¡Ñ‚Ð°Ð²Ð¸Ð¼ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ñ‹Ðµ Ð²ÐµÑ€ÑÐ¸Ð¸ Ð´Ð»Ñ CUDA 11.7 (P100)\n!pip install torch==2.0.1+cu117 torchvision==0.15.2+cu117 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu117\n\n# ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ whisper\n!pip install --upgrade git+https://github.com/openai/whisper.git\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install fastapi==0.103.2 \"uvicorn[standard]==0.23.2\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install \"uvicorn[standard]==0.23.2\" fastapi==0.103.2 --force-reinstall\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ngrok authtoken 2ttGu5JNnkTFeQy3QoilPOFwAvV_6hNru66rgjjPERy3aYZb8\nimport nest_asyncio\nnest_asyncio.apply()\n\nimport uvicorn\nfrom pyngrok import ngrok\nfrom fastapi import FastAPI, UploadFile, File, HTTPException, Form\nfrom fastapi.responses import JSONResponse\nimport os, shutil, requests, subprocess, yt_dlp\n\n# â”€â”€â”€ Hugging Face Summarizers (CPU only) â”€â”€â”€\nfrom transformers import pipeline\nfrom langdetect import detect\n\nsummarizer_en = pipeline(\n    \"summarization\",\n    model=\"facebook/bart-large-cnn\",\n    device=-1   # CPU\n)\nsummarizer_ru = pipeline(\n    \"summarization\",\n    model=\"csebuetnlp/mT5_multilingual_XLSum\",\n    device=-1   # CPU\n)\n\n# â”€â”€â”€ Whisper (GPU small) â”€â”€â”€\nimport torch, whisper\n\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"Device:\", torch.cuda.get_device_name(0))\n\n# Ð³Ñ€ÑƒÐ·Ð¸Ð¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ\nwhisper_model = whisper.load_model(\"small\", device=\"cuda\")\nprint(\"âœ… Whisper small GPU Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½\")\n\ndef whisper_transcribe(file_path: str) -> str:\n    result = whisper_model.transcribe(file_path)\n    return result[\"text\"]\n\n\n# â”€â”€â”€ Utils â”€â”€â”€\nUPLOAD_FOLDER = \"uploads\"\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nMAX_FILE_SIZE = 100 * 1024 * 1024  # 100 MB\n\ndef convert_to_wav(input_path: str) -> str:\n    base, _ = os.path.splitext(input_path)\n    output_path = base + \".wav\"\n    subprocess.run(\n        [\"ffmpeg\", \"-y\", \"-i\", input_path, \"-ar\", \"16000\", \"-ac\", \"1\", output_path],\n        check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n    )\n    return output_path\n\n# ðŸ”¥ Ð¤Ð¸ÐºÑ: Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸Ðµ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð´ÐµÑ€Ð³Ð°Ñ‚ÑŒ yt-dlp Ð½Ð° mp4/mp3\nALLOWED_DIRECT_EXTS = (\".mp4\", \".mp3\", \".wav\", \".m4a\", \".ogg\", \".mkv\", \".avi\", \".mov\")\n\ndef download_media(source_url: str, output_dir=\"uploads\") -> str:\n    os.makedirs(output_dir, exist_ok=True)\n\n    # ÐµÑÐ»Ð¸ ÑÑÑ‹Ð»ÐºÐ° Ð²ÐµÐ´ÐµÑ‚ Ð¿Ñ€ÑÐ¼Ð¾ Ð½Ð° Ñ„Ð°Ð¹Ð» â†’ ÐºÐ°Ñ‡Ð°ÐµÐ¼ requests\n    if any(source_url.lower().endswith(ext) for ext in ALLOWED_DIRECT_EXTS):\n        filename = os.path.basename(source_url.split(\"?\")[0]) or \"downloaded_file\"\n        file_path = os.path.join(output_dir, filename)\n        with requests.get(source_url, stream=True) as r:\n            r.raise_for_status()\n            with open(file_path, \"wb\") as f:\n                shutil.copyfileobj(r.raw, f)\n        return file_path\n\n    # Ð¸Ð½Ð°Ñ‡Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ yt-dlp\n    output_path = os.path.join(output_dir, \"input.%(ext)s\")\n    ydl_opts = {\n        \"format\": \"bestaudio/best\",\n        \"outtmpl\": output_path,\n        \"quiet\": True,\n        \"noplaylist\": True,\n    }\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        info = ydl.extract_info(source_url, download=True)\n        return ydl.prepare_filename(info)\n\n# â”€â”€â”€ Chunk helper â”€â”€â”€\ndef chunk_text(text, max_len=800):\n    words = text.split()\n    chunks, cur = [], []\n    for w in words:\n        cur.append(w)\n        if len(\" \".join(cur)) > max_len:\n            chunks.append(\" \".join(cur))\n            cur = []\n    if cur:\n        chunks.append(\" \".join(cur))\n    return chunks\n\ndef generate_summary(text: str) -> str:\n    try:\n        lang = detect(text)\n    except:\n        lang = \"en\"\n\n    summarizer = summarizer_ru if lang == \"ru\" else summarizer_en\n\n    chunks = chunk_text(text, max_len=800)\n    summary_parts = []\n    for ch in chunks:\n        out = summarizer(ch, max_length=200, min_length=50, do_sample=False)\n        summary_parts.append(out[0][\"summary_text\"])\n    return \" \".join(summary_parts)\n\n# â”€â”€â”€ FastAPI â”€â”€â”€\napp = FastAPI()\n\n@app.post(\"/transcribe\")\nasync def transcribe(\n    file: UploadFile = File(None),\n    source_url: str = Form(None),\n    do_summary: bool = Form(True),\n    session_id: str = Form(\"default\")\n):\n    session_folder = os.path.join(UPLOAD_FOLDER, session_id)\n    os.makedirs(session_folder, exist_ok=True)\n\n    if source_url:\n        try:\n            file_path = download_media(source_url, session_folder)\n        except Exception as e:\n            raise HTTPException(status_code=400, detail=f\"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ð¸: {e}\")\n    elif file:\n        file.file.seek(0, 2)\n        size = file.file.tell()\n        file.file.seek(0)\n        if size > MAX_FILE_SIZE:\n            raise HTTPException(status_code=400, detail=f\"Ð¤Ð°Ð¹Ð» ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¹: {size} Ð±Ð°Ð¹Ñ‚\")\n        file_path = os.path.join(session_folder, file.filename)\n        with open(file_path, \"wb\") as buffer:\n            shutil.copyfileobj(file.file, buffer)\n    else:\n        raise HTTPException(status_code=400, detail=\"ÐÑƒÐ¶Ð½Ð¾ Ð¿Ñ€Ð¸ÑÐ»Ð°Ñ‚ÑŒ Ñ„Ð°Ð¹Ð» Ð¸Ð»Ð¸ ÑÑÑ‹Ð»ÐºÑƒ\")\n\n    try:\n        wav_path = convert_to_wav(file_path)\n        text = whisper_transcribe(wav_path)\n\n        summary = None\n        if do_summary and text.strip():\n            summary = generate_summary(text)\n\n        return {\"text\": text, \"summary\": summary, \"session_id\": session_id}\n    except Exception as e:\n        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n\n# â”€â”€â”€ Ngrok â”€â”€â”€\npublic_url = ngrok.connect(8000)\nprint(\"ðŸ”— Public URL Ð´Ð»Ñ Telegram-Ð±Ð¾Ñ‚Ð°:\", public_url)\n\nuvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-20T02:05:15.330996Z","iopub.execute_input":"2025-09-20T02:05:15.331726Z","iopub.status.idle":"2025-09-20T02:33:30.279548Z","shell.execute_reply.started":"2025-09-20T02:05:15.331692Z","shell.execute_reply":"2025-09-20T02:33:30.278685Z"}},"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cpu\n/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nDevice set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"CUDA available: True\nDevice: Tesla T4\nâœ… Whisper small GPU Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½\nðŸ”— Public URL Ð´Ð»Ñ Telegram-Ð±Ð¾Ñ‚Ð°: NgrokTunnel: \"https://5161fd3f32a5.ngrok-free.app\" -> \"http://localhost:8000\"\n","output_type":"stream"},{"name":"stderr","text":"INFO:     Started server process [259]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\nYour max_length is set to 200, but your input_length is only 172. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=86)\nYour max_length is set to 200, but your input_length is only 181. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=90)\nYour max_length is set to 200, but your input_length is only 20. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n","output_type":"stream"},{"name":"stdout","text":"INFO:     95.82.70.6:0 - \"POST /transcribe HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"INFO:     Shutting down\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\nINFO:     Finished server process [259]\n","output_type":"stream"}],"execution_count":2}]}